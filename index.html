<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ansel Blume</title>

  <meta name="author" content="Ansel Blume">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Ansel Blume
                  </p>

                  <p>
                    Hello! I'm a PhD candidate in Computer Science at the <a href="https://cs.illinois.edu/">University of Illinois Urbana-Champaign</a>
                    where I am advised by <a href="https://blender.cs.illinois.edu/hengji.html">Heng Ji</a> and <a href="https://dhoiem.cs.illinois.edu">Derek Hoiem</a>.
                  </p>
                  <p>
                    Previously, I graduated from <a href="https://ucsd.edu//">UC San Diego</a> with majors in Computer Science and Applied Mathematics, where I worked with
                    <a href="https://cseweb.ucsd.edu/~savage/">Stefan Savage</a> and <a href="https://cseweb.ucsd.edu/~voelker/">Geoff Voelker</a>
                    in the <a href="https://www.sysnet.ucsd.edu/sysnet/">Systems and Networking Group</a>.
                  </p>
                  <p>
                    In my free time I enjoy dancing, which recently has included Zumba (I taught for a year) and dancesport.
                    I used to <a href="https://www.youtube.com/watch?v=xz__V38Gx_g">play Pok√©mon competitively</a> (VGC) and was interested in optimal stat distributions,
                    having written an <a href="https://nuggetbridge.com/articles/bulking-guide-defensive-ev-spreads/">article</a>
                    and <a href="https://www.npmjs.com/package/survivalcalc">calculator</a> for it.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:blume5@illinois.edu">Email</a> &nbsp;/&nbsp;
                    <!-- <a href="data/Ansel Blume CV.pdf">CV</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=76677kkAAAAJ">Google Scholar</a> &nbsp;/&nbsp;

                    <a href="https://github.com/anselblume/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <!-- div for gray background -->
                  <div style="width: 250px; height: 250px; background: #eceaea; border-radius: 50%; display: flex; align-items: center;">
                    <a href="images/anselblume.jpg">
                      <img src="images/anselblume.jpg" alt="profile photo" style="width:90%; border-radius: 50%; border: 0px solid #000000; margin: 0 auto; display: block;">
                    </a>
                  </div>
                  <!-- <a href="images/anselblume.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%; border: 1px solid #000000;" alt="profile photo"
                      src="images/anselblume.jpg" class="hoverZoomLink">
                  </a> -->
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                    <p>
                      My work is broadly in <strong>vision-language</strong> and <strong>decomposable object recognition</strong>.

                      <br><br>

                      Recently, this has included developing a multimodal large language model which segments parts, introducing a training-free method for few-shot object detection, and building a system which uses symbolic object representations with knowledge-graphs and part decomposition for image recognition.

                      <br><br>

                      I am especially interested in improving LLMs' capabilities to reason about problems not easily
                      represented in language (e.g. reasoning about structured state spaces with applications in robotics).
                    </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
            <tbody>

              <tr class="highlighted-paper">
                <td class="image-box">
                  <img src="images/partonomy.jpg" alt="Search and Detect">
                </td>
                <td class="paper-box">
                  <a href="https://arxiv.org/abs/2505.20759">
                    <span class="papertitle">PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding</span>
                  </a>

                  <br>

                  <strong>Ansel Blume</strong>*,
                  Jeonghwan Kim*,
                  Hyeonjeong Ha,
                  Elen Chatikyan,
                  Xiaomeng Jin,
                  Khanh Duy Nguyen,
                  Nanyun Peng,
                  Kai-Wei Chang,
                  Derek Hoiem,
                  Heng Ji

                  <br><br>

                  <em>Preprint</em>

                  <br>

                  <a href="https://arxiv.org/abs/2505.20759">arXiv</a> / Code and dataset coming soon!
                  <p></p>
                  <p>
                    Large multimodal models (LMMs) with the ability to segment, segmenting LMMs, have poor part understanding. We introduce the <emph>explanatory part segmentation</emph> task and construct the Partonomy part understanding dataset to evaluate it. We use Partonomy to train PLUM, a segmenting LMM with strong part segmentation abilities which outperforms other segmenting LMMs on general reasoning and segmentation tasks.
                  </p>
                </td>
              </tr>

              <tr class="highlighted-paper">
                <td class="image-box">
                  <img src="images/search_and_detect.jpg" alt="Search and Detect">
                </td>
                <td class="paper-box">
                  <a href="https://arxiv.org/abs/2409.18733">
                    <span class="papertitle">Search and Detect: Training-Free Long Tail Object Detection via Web-Image Retrieval</span>
                  </a>

                  <br>

                  Mankeerat Sidhu,
                  Hetarth Chopra,
                  <strong>Ansel Blume</strong>,
                  Jeonghwan Kim,
                  Revanth Gangi Reddy,
                  Heng Ji

                  <br><br>

                  <em>CVPR</em>, 2025

                  <br>

                  <a href="https://arxiv.org/abs/2409.18733">arXiv</a>
                  <p></p>
                  <p>
                    Exemplar images from the web can be used for highly effective training-free long-tail object detection by combining embedding heatmaps with SAM regions.
                    This method far surpasses SOTA few-shot methods on several benchmarks.
                  </p>
                </td>
              </tr>

              <tr>
                <td class="image-box">
                  <img src="images/miracle.jpg" alt="MIRACLE">
                </td>
                <td class="paper-box">
                  <a href="https://dl.acm.org/doi/10.1145/3664647.3684993">
                    <span class="papertitle">MIRACLE: An Online, Explainable Multimodal Interactive Concept Learning System</span>
                  </a>

                  <br>

                  <strong>Ansel Blume</strong>*,
                  Khanh Duy Nguyen*,
                  Zhenhailong Wang,
                  Yangyi Chen,
                  Michal Shlapentokh-Rothman,
                  Xiaomeng Jin,
                  Jeonghwan Kim,
                  Zhen Zhu,
                  Jiateng Liu,
                  Kuan-Hao Huang,
                  Mankeerat Sidhu,
                  Xuanming Zhang,
                  Vivian Liu,
                  Raunak Sinha,
                  Te-Lin Wu,
                  Abhay Zala,
                  Elias Stengel-Eskin,
                  Da Yin,
                  Yao Xiao,
                  Utkarsh Mall,
                  Zhou Yu,
                  Kai-Wei Chang,
                  Camille Cobb,
                  Karrie Karahalios,
                  Lydia Chilton,
                  Mohit Bansal,
                  Nanyun Peng,
                  Carl Vondrick,
                  Derek Hoiem,
                  Heng Ji

                  <br><br>

                  <em>ACM MM Technical Demos</em>, 2024

                  <br>

                  <a href="https://dl.acm.org/doi/10.1145/3664647.3684993">ACM Page</a> / <a href="https://github.com/AnselBlume/ecole_miracle_mo9">Github</a>
                  <p></p>
                  <p>
                    We developed MIRACLE, an interactive system for object recognition that learns concepts in real-time,
                    highlighting key regions that distinguish objects from one another.
                  </p>
                </td>
              </tr>

              <tr class="highlighted-paper">
                <td class="image-box">
                  <img src="images/region_reps.jpg" alt="Region-based Representations Revisited">
                </td>
                <td class="paper-box">
                  <a href="https://arxiv.org/abs/2402.02352">
                    <span class="papertitle">Region-based Representations Revisited</span>
                  </a>

                  <br>

                  Michal Shlapentokh-Rothman*,
                  <strong>Ansel Blume</strong>*,
                  Yao Xiao,
                  Yuqun Wu,
                  Sethuraman TV,
                  Heyi Tao,
                  Jae Yong Lee,
                  Wilfredo Torres,
                  Yu-Xiong Wang,
                  Derek Hoiem

                  <br><br>

                  <em>CVPR</em>, 2024

                  <br>

                  <a href="https://arxiv.org/abs/2402.02352">arXiv</a> / <a href="https://regionreps.web.illinois.edu/">Project Page</a>
                  <p></p>
                  <p>
                    Region features constructed by average pooling image features over SAM regions are effective on a wide range of downstream tasks.
                  </p>
                </td>
              </tr>

              <tr>
                <td class="image-box">
                  <img src="images/product_attributes.jpg" alt="Generative Models for Product Attribute Extraction">
                </td>
                <td class="paper-box">
                  <a href="https://aclanthology.org/2023.emnlp-industry.55/">
                    <span class="papertitle">Generative Models for Product Attribute Extraction</span>
                  </a>

                  <br>

                  <strong>Ansel Blume</strong>,
                  Nasser Zalmout,
                  Heng Ji,
                  Xian Li

                  <br><br>

                  <em>EMNLP Industry Track</em>, 2023

                  <br>

                  <a href="https://aclanthology.org/2023.emnlp-industry.55/">ACL Page</a>
                  <p></p>
                  <p>
                    Generative language models can outperform extractive product attribute extraction models while having greater
                    data efficiency and the unique ability to detect implied attributes.
                  </p>
                </td>
              </tr>

              <tr>
                <td class="image-box">
                  <img src="images/paxion.jpg" alt="Paxion: Patching Action Knowledge in Video-Language Foundation Models">
                </td>
                <td class="paper-box">
                  <a href="https://arxiv.org/abs/2305.10683">
                    <span class="papertitle">Paxion: Patching Action Knowledge in Video-Language Foundation Models</span>
                  </a>

                  <br>

                  Zhenhailong Wang,
                  <strong>Ansel Blume</strong>,
                  Sha Li,
                  Genglin Liu,
                  Jaemin Cho,
                  Zineng Tang,
                  Mohit Bansal,
                  Heng Ji

                  <br><br>

                  <em>NeurIPS</em>, 2023

                  <br>

                  <a href="https://arxiv.org/abs/2305.10683">arXiv</a> / <a href="https://github.com/MikeWangWZHL/Paxion">Github</a>
                  <p></p>
                  <p>
                    Video-language foundation models are highly biased towards using objects for action recognition, as opposed to actually
                    analyzing the action itself. Paxion proposes a training scheme that improves action recognition without harming performance on downstream tasks.
                  </p>
                </td>
              </tr>

              <tr>
                <td class="image-box">
                  <img src="images/measuring_security.jpg" alt="Measuring Security Practices and How They Impact Security">
                </td>
                <td class="paper-box">
                  <a href="https://dl.acm.org/doi/10.1145/3355369.3355571">
                    <span class="papertitle">Measuring Security Practices and How They Impact Security</span>
                  </a>

                  <br>

                  Louis F. DeKoven,
                  Audrey Randall,
                  Ariana Mirian,
                  Gautam Akiwate,
                  <strong>Ansel Blume</strong>,
                  Lawrence K. Saul,
                  Aaron Schulman,
                  Geoffrey M. Voelker,
                  Stefan Savage

                  <br><br>

                  <em>IMC</em>, 2019

                  <br>

                  <a href="https://dl.acm.org/doi/10.1145/3355369.3355571">ACM Page</a>
                  <p></p>
                  <p>
                    A large scale study on factors and security practices that help to prevent system compromise in practice.
                  </p>
                </td>
              </tr>

              <!-- <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='simvs_image'><video width=100% muted autoplay loop>
                        <source src="images/simvs.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/simvs.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function simvs_start() {
                      document.getElementById('simvs_image').style.opacity = "1";
                    }

                    function simvs_stop() {
                      document.getElementById('simvs_image').style.opacity = "0";
                    }
                    simvs_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://nerf-casting.github.io/">
                    <span class="papertitle">SimVS: Simulating World Inconsistencies for Robust View Synthesis</span>
                  </a>
                  <br>
                  <a href="https://alextrevithick.github.io/">Alex Trevithick</a>,
                  <a href="https://scholar.google.com/citations?user=-KSDNZQAAAAJ&hl=en">Roni Paiss</a>,
                  <a href="https://henzler.github.io/">Philipp Henzler</a>,
                  <a href="https://dorverbin.github.io/">Dor Verbin</a>,
                  <a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu</a>,
                  <a href="https://hadizayer.github.io/">Hadi Alzayer</a>,
                  <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
                  <a href="https://poolio.github.io/">Ben Poole</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://holynski.org/">Aleksander Holynski</a>,
                  <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>
                  <br>
                  <em>arXiv</em>, 2024
                  <br>
                  <a href="https://alextrevithick.github.io/simvs/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2412.07696">arXiv</a>
                  <p></p>
                  <p>
                    Simulating the world with video models lets you make inconsistent captures consistent.
                  </p>
                </td>
              </tr> -->

              <!-- <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cat4d_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/cat4d.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/cat4d.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function cat4d_start() {
                      document.getElementById('cat4d_image').style.opacity = "1";
                    }

                    function cat4d_stop() {
                      document.getElementById('cat4d_image').style.opacity = "0";
                    }
                    cat4d_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://cat-4d.github.io/">
                    <span class="papertitle">CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models
                    </span>
                  </a>
                  <br>
                  <a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu</a>,
                  <a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
                  <a href="https://poolio.github.io/">Ben Poole</a>,
                  <a href="https://alextrevithick.github.io/">Alex Trevithick</a>,
                  <a href="https://www.cs.columbia.edu/~cxz/index.htm/">Changxi Zheng</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://holynski.org/">Aleksander Holynski</a>
                  <br>
                  <em>arXiv</em>, 2024
                  <br>
                  <a href="https://cat-4d.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2411.18613">arXiv</a>
                  <p></p>
                  <p>
                    An approach for turning a video into a 4D radiance field that can be rendered in real-time. When
                    combined with a text-to-video model, this enables text-to-4D.
                  </p>
                </td>
              </tr> -->

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Many thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the
                    <a href="https://github.com/jonbarron/jonbarron_website">website template!</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
      </tabler </body>

</html>